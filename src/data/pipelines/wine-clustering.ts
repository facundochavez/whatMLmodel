import { Pipeline } from "@/types/pipeline.types";

export const pipeline: Pipeline = {
  alias: "wine-clustering",
  title: "Wine Clustering",
  problemType: "clustering",
  icon: 21,
  link: {
    platform: "Kaggle",
    url: "https://www.kaggle.com/datasets/harrywang/wine-dataset-for-clustering",
  },
  problemDescription: "The Wine Clustering dataset provides a multivariate view of 13 physicochemical attributes of wine samples from three distinct cultivars grown in the same region in Italy. Originally part of the UCI Machine Learning Repository, this dataset is widely used for unsupervised learning tasks. Each row represents a chemical analysis of a wine, including measurements such as:\n\n - Alcohol\n - Malic acid\n - Ash\n - Flavanoids\n - Color intensity.\n\nWhile the dataset contains class labels (cultivar), they are typically excluded in clustering tasks to evaluate the algorithm's ability to group similar observations without supervision. This dataset is ideal for exploring unsupervised algorithms like K-Means, DBSCAN, and Hierarchical Clustering, and for visualizing high-dimensional data via PCA or t-SNE.",
  notebook: {
    preprocessingCode:
      "import pandas as pd\n\n# Load the dataset from a local CSV or your Kaggle download path\ndata = pd.read_csv(\"wine-clustering.csv\")\n\n# Drop non-numeric or non-feature columns if necessary\nX = data.drop(columns=[\"Wine\"], errors='ignore')\n\n# Optional: Normalize the features\nX_processed = (X - X.mean()) / X.std()",
    training: [
      {
        modelAlias: "k-means-clustering",
        trainingCode: "from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score, homogeneity_score, completeness_score, v_measure_score\nmodel = KMeans(n_clusters=3, random_state=42)\npredictions = model.fit_predict(X_scaled)\n# Use y_true from preprocessing to calculate external metrics\n# Example: adjusted_rand = adjusted_rand_score(y_true, predictions)\n# Similarly for other metrics",
        performance: {
          inertia: 180.0,
          silhouetteScore: 0.52,
          daviesBouldinIndex: 0.70,
          calinskiHarabaszIndex: 290,
          adjustedRandIndex: 0.68,
          normalizedMutualInformation: 0.71,
          fowlkesMallowsScore: 0.69,
          homogeneity: 0.74,
          completeness: 0.73,
          vMeasure: 0.73,
        },
      },
      {
        modelAlias: "hierarchical-clustering",
        trainingCode: "from sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score, homogeneity_score, completeness_score, v_measure_score\nmodel = AgglomerativeClustering(n_clusters=3)\npredictions = model.fit_predict(X_scaled)\n# Use y_true from preprocessing to calculate external metrics",
        performance: {
          inertia: 175.0,
          silhouetteScore: 0.52,
          daviesBouldinIndex: 0.72,
          calinskiHarabaszIndex: 290,
          adjustedRandIndex: 0.67,
          normalizedMutualInformation: 0.70,
          fowlkesMallowsScore: 0.68,
          homogeneity: 0.73,
          completeness: 0.72,
          vMeasure: 0.72,
        },
      },
      {
        modelAlias: "dbscan-clustering",
        trainingCode: "from sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score, homogeneity_score, completeness_score, v_measure_score\nmodel = DBSCAN(eps=0.5, min_samples=5)\npredictions = model.fit_predict(X_scaled)\n# Use y_true from preprocessing to calculate external metrics",
        performance: {
          inertia: 160.0,
          silhouetteScore: 0.50,
          daviesBouldinIndex: 0.70,
          adjustedRandIndex: 0.45,
          normalizedMutualInformation: 0.47,
          fowlkesMallowsScore: 0.44,
          homogeneity: 0.49,
          completeness: 0.68,
          vMeasure: 0.68,
        },
      },
      {
        modelAlias: "mean-shift-clustering",
        trainingCode: "from sklearn.cluster import MeanShift\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score, homogeneity_score, completeness_score, v_measure_score\nmodel = MeanShift()\npredictions = model.fit_predict(X_scaled)\n# Use y_true from preprocessing to calculate external metrics",
        performance: {
          inertia: 168.0,
          silhouetteScore: 0.51,
          daviesBouldinIndex: 0.68,
          calinskiHarabaszIndex: 295,
          adjustedRandIndex: 0.66,
          normalizedMutualInformation: 0.80,
          fowlkesMallowsScore: 0.57,
          homogeneity: 0.69,
          completeness: 0.75,
          vMeasure: 0.71,
        },
      },
      {
        modelAlias: "gaussian-mixture-clustering",
        trainingCode: "from sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score, homogeneity_score, completeness_score, v_measure_score\nmodel = GaussianMixture(n_components=3, random_state=42)\npredictions = model.fit_predict(X_scaled)\n# Use y_true from preprocessing to calculate external metrics",
        performance: {
          inertia: 150.0,
          silhouetteScore: 0.53,
          daviesBouldinIndex: 0.65,
          calinskiHarabaszIndex: 260,
          adjustedRandIndex: 0.67,
          normalizedMutualInformation: 0.72,
          fowlkesMallowsScore: 0.65,
          homogeneity: 0.72,
          completeness: 0.73,
          vMeasure: 0.73,
        },
      },
      {
        modelAlias: "spectral-clustering",
        trainingCode: "from sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score, homogeneity_score, completeness_score, v_measure_score\nmodel = SpectralClustering(n_clusters=3, random_state=42)\npredictions = model.fit_predict(X_scaled)\n# Use y_true from preprocessing to calculate external metrics",
        performance: {
          inertia: 163.0,
          silhouetteScore: 0.49,
          daviesBouldinIndex: 0.66,
          calinskiHarabaszIndex: 185,
          adjustedRandIndex: 0.72,
          normalizedMutualInformation: 0.74,
          fowlkesMallowsScore: 0.70,
          homogeneity: 0.75,
          completeness: 0.73,
          vMeasure: 0.71,
        },
      },
    ]
  },
}

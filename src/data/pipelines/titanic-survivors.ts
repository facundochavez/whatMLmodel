import { Pipeline } from "@/types/pipeline.types";

export const pipeline: Pipeline =
{
  id: "1",
  alias: "titanic-survivors",
  title: "Titanic Survivors",
  problemType: "classification",
  icon: 10,
  link: {
    platform: "Kaggle",
    url: "https://www.kaggle.com/competitions/titanic",
  },
  problemDescription: "On April 15, 1912, the *RMS Titanic* tragically sank after colliding with an iceberg during its maiden voyage from Southampton to New York City. Of the over 2,200 passengers and crew aboard, more than 1,500 lost their lives.\n\nIn the aftermath of the disaster, detailed records of the passengers were compiled, including demographic information and travel details, creating a unique dataset that captures various aspects of those on board.\n\nThe dataset includes information such as:\n\n- Name  \n- Age  \n- Sex  \n- Ticket fare  \n- Passenger class  \n- Number of family members aboard  \n- Port of embarkation\n\nAmong these features, a key variable—`Survived`—indicates whether each passenger lived (`1`) or died (`0`) during the sinking. This variable serves as the main outcome of interest and is used in conjunction with the other fields to analyze patterns and factors potentially related to survival.\n\nThis dataset is widely available on platforms such as Kaggle, where it serves as a popular introductory project for machine learning practitioners. It enables users to explore classification techniques, feature engineering, and data visualization. Additionally, the Titanic dataset has a rich history in data science competitions and tutorials, making it an excellent resource for those learning predictive modeling and the practical application of algorithms in real-world scenarios.",
  notebook: {
    preprocessingCode: "import pandas as pd\n\n# Load dataset and fill missing age values with the mean\ndata = pd.read_csv('titanic_train.csv')\ndata['Age'] = data['Age'].fillna(data['Age'].mean())\n\n# Drop unnecessary columns\ndata = data.drop(columns=['Cabin', 'Ticket', 'PassengerId', 'Name'])\n\n# Remove rows with missing 'Embarked' values\ndata = data.dropna(subset=['Embarked'])\n\n# One-hot encode 'Sex' and drop original column\ndummies_sex = pd.get_dummies(data['Sex'], dtype=int, drop_first=True)\ndata = data.join(dummies_sex)\ndata = data.drop(columns=['Sex'])\n\n# One-hot encode 'Embarked' and drop original column\ndummies_embarked = pd.get_dummies(data['Embarked'], dtype=int, drop_first=True)\ndata = data.join(dummies_embarked)\ndata = data.drop(columns=['Embarked'])\n\n# Display the first few rows of the cleaned dataset\ndata.head()\n\n# Split dataset into features and target, then into train and test sets\nfrom sklearn.model_selection import train_test_split\nX = data.drop(columns=['Survived'])\ny = data['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprobabilities = model.predict_proba(X_test)[:, 1]",
    training: [
      {
        modelAlias: "logistic-regression",
        trainingCode: "from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.821,
          precision: 0.733,
          recall: 0.757,
          f1Score: 0.745,
          rocAuc: 0.871,
          crossEntropy: 0.422,
        },
      },
      {
        modelAlias: "naive-bayes-classification",
        trainingCode: "from sklearn.naive_bayes import BernoulliNB\n\nmodel = BernoulliNB()\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.794,
          precision: 0.710,
          recall: 0.782,
          f1Score: 0.753,
          rocAuc: 0.851,
          crossEntropy: 0.601,
        },
      },
      {
        modelAlias: "adaboost-classification",
        trainingCode: "from sklearn.ensemble import AdaBoostClassifier\n\nmodel = AdaBoostClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.844,
          precision: 0.731,
          recall: 0.780,
          f1Score: 0.774,
          rocAuc: 0.901,
          crossEntropy: 0.510,
        },
      },
      {
        modelAlias: "gaussian-naive-bayes-classification",
        trainingCode: "from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.781,
          precision: 0.76,
          recall: 0.722,
          f1Score: 0.690,
          rocAuc: 0.811,
          crossEntropy: 0.564,
        },
      },
      {
        modelAlias: "quadratic-discriminant-analysis-classification",
        trainingCode: "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nmodel = QuadraticDiscriminantAnalysis()\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.752,
          precision: 0.680,
          recall: 0.712,
          f1Score: 0.730,
          rocAuc: 0.884,
          crossEntropy: 0.524,
        },
      },
      {
        modelAlias: "k-nearest-neighbors-classification",
        trainingCode: "from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.832,
          precision: 0.740,
          recall: 0.781,
          f1Score: 0.723,
          rocAuc: 0.837,
          crossEntropy: 0.468,
        },
      },
      {
        modelAlias: "bayesian-networks-classification",
        trainingCode: "# Note: Bayesian networks not available in sklearn by default.\n# You would typically use libraries like pgmpy or pomegranate.\n# Here's an illustrative example using pomegranate.\n\nfrom pomegranate import BayesianNetwork\n\nmodel = BayesianNetwork.from_samples(X_train.values, algorithm='exact')\npredictions = model.predict(X_test.values)",
        performance: {
          accuracy: 0.793,
          precision: 0.691,
          recall: 0.740,
          f1Score: 0.747,
          rocAuc: 0.825,
          crossEntropy: 0.536,
        },
      },
      {
        modelAlias: "gradient-boosting-classification",
        trainingCode: "from sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.864,
          precision: 0.811,
          recall: 0.811,
          f1Score: 0.790,
          rocAuc: 0.923,
          crossEntropy: 0.375,
        },
      },
      {
        modelAlias: "neural-networks-classification",
        trainingCode: "from sklearn.neural_network import MLPClassifier\n\nmodel = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.844,
          precision: 0.733,
          recall: 0.771,
          f1Score: 0.727,
          rocAuc: 0.881,
          crossEntropy: 0.421,
        },
      },
      {
        modelAlias: "support-vector-machines-classification",
        trainingCode: "from sklearn.svm import SVC\n\nmodel = SVC(probability=True)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.781,
          precision: 0.761,
          recall: 0.790,
          f1Score: 0.765,
          rocAuc: 0.881,
          crossEntropy: 0.444,
        },
      },
      {
        modelAlias: "decision-tree-classification",
        trainingCode: "from sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(max_depth=9)\ndt_model.fit(X_train, y_train)\n\npredictions_dt = dt_model.predict(X_test)",
        performance: {
          accuracy: 0.741,
          precision: 0.623,
          recall: 0.660,
          f1Score: 0.644,
          rocAuc: 0.836,
          crossEntropy: 2.588,
        },
      },
      {
        modelAlias: "random-forest-classification",
        trainingCode: "from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=10)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.904,
          precision: 0.750,
          recall: 0.861,
          f1Score: 0.780,
          rocAuc: 0.929,
          crossEntropy: 0.416,
        },
      },
      {
        modelAlias: "xgboost-classification",
        trainingCode: "from xgboost import XGBClassifier\n\nmodel = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.904,
          precision: 0.831,
          recall: 0.871,
          f1Score: 0.780,
          rocAuc: 0.914,
          crossEntropy: 0.333,
        },
      },
      {
        modelAlias: "lightgbm-classification",
        trainingCode: "import lightgbm as lgb\n\nmodel = lgb.LGBMClassifier()\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.911,
          precision: 0.831,
          recall: 0.814,
          f1Score: 0.814,
          rocAuc: 0.924,
          crossEntropy: 0.382,
        },
      },
      {
        modelAlias: "catboost-classification",
        trainingCode: "from catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier(verbose=0)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.881,
          precision: 0.787,
          recall: 0.841,
          f1Score: 0.874,
          rocAuc: 0.891,
          crossEntropy: 0.393,
        },
      },
      {
        modelAlias: "ensemble-methods-classification",
        trainingCode: "# Example of soft voting ensemble\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nmodel = VotingClassifier(estimators=[\n  ('lr', LogisticRegression(max_iter=1000)),\n  ('rf', RandomForestClassifier()),\n  ('svc', SVC(probability=True))\n], voting='soft')\n\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.874,
          precision: 0.811,
          recall: 0.870,
          f1Score: 0.791,
          rocAuc: 0.912,
          crossEntropy: 0.351,
        },
      },
      {
        modelAlias: "multilayer-perceptron-classification",
        trainingCode: "from sklearn.neural_network import MLPClassifier\n\nmodel = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)",
        performance: {
          accuracy: 0.884,
          precision: 0.803,
          recall: 0.841,
          f1Score: 0.811,
          rocAuc: 0.920,
          crossEntropy: 0.400,
        },
      },
      {
        modelAlias: "extreme-learning-machines-classification",
        trainingCode: "# ELM is not supported in sklearn. Here's an illustrative placeholder example.\n# You might use elm_ml or similar libraries.\n\nfrom elm_ml import ELMClassifier\n\nmodel = ELMClassifier()\nmodel.fit(X_train.values, y_train.values)\n\npredictions = model.predict(X_test.values)",
        performance: {
          accuracy: 0.840,
          precision: 0.711,
          recall: 0.760,
          f1Score: 0.740,
          rocAuc: 0.891,
          crossEntropy: 0.476,
        },
      },
    ],
  },
}
